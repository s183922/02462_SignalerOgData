{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound notebook\n",
    "This notebook takes you through some of the basic aspects when analyzing sound. Before we start, we need to make sure, you have the correct python packages. We assume you have anaconda installed. If not, follow the instructions [here](https://www.anaconda.com/distribution/).\n",
    "\n",
    "### Importing and installing packages\n",
    "Beneath are all the packages we need for this exercise. If you get an error looking like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ModuleNotFoundError                       Traceback (most recent call last)**\n",
    "**<ipython-input-8-9625e36b3edc> in <module>()**\n",
    "**----> 1 import pyaudio**\n",
    "**      2 import wave**\n",
    "\n",
    "**ModuleNotFoundError: No module named 'pyaudio'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is because you don't have the package *pyaudio* on your computer, and you need to run *pip install pyaudio* in your commando prompt (windows) or terminal (macOS). If you are on linux and have trouble with accessing your microphone device, either sit together with a windows/mac person or use the \"sofa.wav\" that comes with this .zip file and skip to the second step in Exercise 1 **Plot the time series**. When you have installed the missing packages, you should be able to import them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "from scipy.signal import butter, filtfilt, freqz\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1\n",
    "## Exercise 0: Check that you can record and save wave files from your laptop microphone\n",
    "The pyaudio package lets you record sound from your laptop microphone. We can define some of the properties, such as recording duration, output file name, etc. This is done in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"test_file.wav\"\n",
    "SAMPLING_FREQUENCY = 44100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss choice of sampling frequency.\n",
    "- What is the highest frequency that can be represented this a sampling frequency?\n",
    "\n",
    "Now, test that your recording device works, by executing the code beneath (don't worry about the code itself). Remember to check that your microphone is **on**. While it is running, say something to make sure it captures your beatiful voice :-) After it is done, a file names \"test_file.wav\" should appear in the same folder as you have located this notebook. Check that you have this file when finished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def record_audio(WAVE_OUTPUT_FILENAME, RECORD_SECONDS, SAMPLING_FREQUENCY):\n",
    "    audio = pyaudio.PyAudio()\n",
    "    CHUNK=1024  #size of analysis frame \n",
    "    RATE=SAMPLING_FREQUENCY \n",
    "    CHANNELS=2  # stereo\n",
    "    FORMAT=pyaudio.paInt16\n",
    "    stream = audio.open(format=pyaudio.paInt16, channels=2,\n",
    "                    rate=SAMPLING_FREQUENCY, input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "    print(\"recording...\")\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    print(\"finished recording\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    waveFile.setnchannels(CHANNELS)\n",
    "    waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    waveFile.setframerate(RATE)\n",
    "    waveFile.writeframes(b''.join(frames))\n",
    "    waveFile.close()\n",
    "    \n",
    "#record_audio(WAVE_OUTPUT_FILENAME, RECORD_SECONDS, SAMPLING_FREQUENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Record yourself\n",
    "Record yourself or someone around you to say the english single letter words in sequence **s o f a**, i.e., pronounced as \"s$\\cdot$o$\\cdot$f$\\cdot$a\". The idea is that you say the letters explicitly and slowly. Afterwards, go to the created file (which we shall call \"sofa.wav\") and make sure you can clearly hear what is being said and that it is not too loud. We have allocated 6 seconds to say \"s$\\cdot$o$\\cdot$f$\\cdot$a\", but you can increase this if necessary by changing RECORD_SECONDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WAVE_OUTPUT_FILENAME = \"sofa.wav\"\n",
    "RECORD_SECONDS = 6\n",
    "record_audio(WAVE_OUTPUT_FILENAME,RECORD_SECONDS, SAMPLING_FREQUENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the time series\n",
    "We can now plot the time signal that has been recorded. You should be able to zoom in/out on the signal to explore its nature. \n",
    "\n",
    "- Recognize and describe the components of the individual sounds recorded. \n",
    "- Discuss the duration and the number of samples in relation to the sampling frequency and duration of the recording?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signal = read(\"sofa.wav\")\n",
    "signal = np.array(signal[1][:,0],dtype=float)\n",
    "fig = plt.figure(1)\n",
    "plt.plot(signal)\n",
    "plt.xlabel('Sample');\n",
    "plt.ylabel('Amplitude');\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the time series and spectrogram\n",
    "Let's plot the time signal along with the spectrogram (frequency content over time)\n",
    "\n",
    "We will use optimized estimation of spectrograms for the experiments.  The operation is similar to projecting on periodic function basis vectors, but optimized for speed and performance. In particular it uses optimized methods for estimation of frequency content in overlapping windows (https://www.dspguide.com/ch9/1.htm).\n",
    "\n",
    "- Discuss the information conveyed by the spectrogram? \n",
    "- For which time windows and frequencies is the spectrogram estimated? What are the (matrix) dimensions of spectrogram?\n",
    "Compare with the size of the window and the overlap\n",
    "- Recognize and described the sound components (phonemes) appearing in the spectrogram of the recording \"s$\\cdot$o$\\cdot$f$\\cdot$a\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(2)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(signal);\n",
    "plt.xlabel('Sample');\n",
    "plt.ylabel('Amplitude');\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.specgram(signal,NFFT=256,noverlap=128, Fs=SAMPLING_FREQUENCY, cmap='viridis');\n",
    "# NFFT:  size of window in samples. noverlap: size of overlap between windows in samples. \n",
    "plt.xlabel('Time [sec]');\n",
    "plt.ylabel('Frequency [Hz]');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: (Frequency) Filtering\n",
    "Next we are going to filter signals to enhance certain frequency content. Typical applications involve  low-pass, high-pass, band-stop and band-pass filtering, where a \"band\" refers to a certain frequency range. Low-pass filter can reduce noise, high-pass filters can eliminate slowly varying \"trends\" in signals. Band-stop filters can be used for eliminating certain unwanted frequencies such as generated by a 50Hz power outlet. Band-pass filters can be used to detect specific frequencies, such as the $\\alpha-$rhytm in brain waves. Our experiments in filtering will be based on the own sound recordings. \n",
    "\n",
    "We will use professional grade filters in the experiments, the operation is similar to projecting on periodic functoin basis vectors, but optimized for speed and performance. In particular we will use the so-called butterworth filters (see https://www.dspguide.com/ch20/1.htm and https://en.wikipedia.org/wiki/Butterworth_filter for additional references)that have been optimized for sensitivity to the pass frequencies, while effectively rejecting the stop frequencies.\n",
    "\n",
    "#### Low pass filtering: Let the $low$ frequencies pass!\n",
    "For low pass filtering, we set a \"cutoff frequency\", which defines the boundary between the frequencies being filtered away and the frequencies being kept in the signal. We use a butterworth filter of order 8. Start with a $cutoff=1000$ Hz and look at the corresponding spectrogram. Change the cutoff and discuss the changes to the spectrogram. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "cutoff = 1000  \n",
    "signal_lp_filt = butter_lowpass_filter(signal, cutoff, SAMPLING_FREQUENCY, 8)\n",
    "\n",
    "fig = plt.figure(4)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.specgram(signal, Fs=SAMPLING_FREQUENCY);\n",
    "plt.title(\"Before low pass\")\n",
    "plt.xlabel('Time [sec]');\n",
    "plt.ylabel('Frequency [Hz]');\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.specgram(signal_lp_filt, Fs=SAMPLING_FREQUENCY);\n",
    "plt.title(\"After low pass\")\n",
    "plt.xlabel('Time [sec]');\n",
    "plt.ylabel('Frequency [Hz]');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High pass filtering: Let the $high$ frequencies pass!\n",
    "For high pass filtering, we set --- as before --- a cutoff frequency, which defines our boundary between the frequencies being filtered away and the ones kept. Again, we use a butterworth filter of order 8. Start with a $cutoff=2500$ Hz and look at the corresponding spectrogram. Change the cutoff and see how this changes the spectrogram. \n",
    "\n",
    "- Can you explain what is going on in the spectrogram when changing the cutoff frequency?\n",
    "- How is this different from the low pass filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "def butter_highpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "cutoff = 2500\n",
    "signal_hp_filt = butter_highpass_filter(signal, cutoff, SAMPLING_FREQUENCY, 8)\n",
    "\n",
    "fig = plt.figure(5)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.specgram(signal, Fs=SAMPLING_FREQUENCY);\n",
    "plt.title(\"Before high pass\")\n",
    "plt.xlabel('Time [sec]');\n",
    "plt.ylabel('Frequency [Hz]');\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.specgram(signal_hp_filt, Fs=SAMPLING_FREQUENCY);\n",
    "plt.title(\"After high pass\")\n",
    "plt.xlabel('Time [sec]');\n",
    "plt.ylabel('Frequency [Hz]');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the filtered signals\n",
    "Let us hear the low passed signal and the high passed signal, respectively. Run the 3 next code blocks. \n",
    "\n",
    "- Recognize the filtered versions of signal  and explain the differences to the original recorded signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lp_cutoff = 1000  \n",
    "signal_lp_filt = butter_lowpass_filter(signal, lp_cutoff, SAMPLING_FREQUENCY, 8)\n",
    "hp_cutoff = 2500\n",
    "signal_hp_filt = butter_highpass_filter(signal, hp_cutoff, SAMPLING_FREQUENCY, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio(signal_lp_filt, rate=SAMPLING_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio(signal_hp_filt, rate=SAMPLING_FREQUENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2\n",
    "## Exercise 3: Extract each letter\n",
    "Go to Fig. 1 (or the time series in Fig. 2) and find the start-sample and end-sample for each letter by dragging the mouse over the plot. The x-value is displayed in the bottom right corner. Remember(!) x is **samples** and these are only whole numbers (no decimals). Store the 8 values in the variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_s,end_s = 57000,93910  \n",
    "start_o,end_o = 103871,135510  \n",
    "start_f,end_f = 147814,181211  \n",
    "start_a,end_a = 190000,219295  \n",
    "\n",
    "sound_s = signal[start_s:end_s]\n",
    "sound_o = signal[start_o:end_o]\n",
    "sound_f = signal[start_f:end_f]\n",
    "sound_a = signal[start_a:end_a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot each letter\n",
    "Lets plot the time series for each letter. \n",
    "- Recognize the sounds making up  these single letter words. \n",
    "- Discuss the frequency content of the two components of the  $s$-sound. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(3)\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.title(\"S\")\n",
    "plt.plot(sound_s);\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.title(\"o\")\n",
    "plt.plot(sound_o);\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.title(\"f\")\n",
    "plt.plot(sound_f);\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.title(\"a\")\n",
    "plt.plot(sound_a);\n",
    "\n",
    "plt.xlabel('Samples');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play each sound\n",
    "Play the sounds to check you successfully separated them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio(sound_s, rate=SAMPLING_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio(sound_o, rate=SAMPLING_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio(sound_f, rate=SAMPLING_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio(sound_a, rate=SAMPLING_FREQUENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Find your fundamental frequency (grundtone)\n",
    "Speech is a sequence of voice and unvoiced sounds, produced by the  speech organs. The voiced sounds are produced by a vibrating vocal cord, hence, presents both a fundamental frequency and high harmonics. The fundamental frequency is also referred to as the pitch.  What creates each specific sound is the combination of harmonics the power disribution among them. This could also be seen from the spectrograms of the different sounds in \"sofa\"; each letter has a special frequency composition and when put together we can make words and sentences. However, imagine you recorded a deep voice saying \"sofa\" and a high voice saying \"sofa\"; they say have the same semantics yet, sound very different. \n",
    "\n",
    "From the time series of your own data, we will next measure your pitch in Hertz (Hertz is the number of cycles pr. second). Estimate your pitch frequency by visual inspection of the time series in Fig. 3. Zoom in on on a part of the signal. Estimate the uncertainty of the measured frequency, possibly by repeting the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "est_pitch = [] # put your answer here in Hz\n",
    "# Solution\n",
    "#print(str(est_pitch) + \" Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pitch is highly variable in the population with some differences between the genders, and variability due to physical height differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Extract phonemes\n",
    "The pronunciation of the single letter words $s$ and $f$ have one phoneme (sound component) in common. The phoneme is often listed with the symbol: $/\\epsilon /$.\n",
    "Extract the first phoneme in $s$ and $f$ by same method as used before: visual inspection and finding sample(s). Use Fig. 3 for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_s_e,end_s_e = 4210,14224\n",
    "start_f_e,end_f_e = 3800,13081\n",
    "\n",
    "sound_s_e = sound_s[start_s_e:end_s_e]\n",
    "sound_f_e = sound_s[start_f_e:end_f_e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hear if they sound the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio(sound_s_e, rate=SAMPLING_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio(sound_f_e, rate=SAMPLING_FREQUENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 \n",
    "Next we manipulate sounds to illustrate the component structure. Separate the two phonemes of the s and f words and recombined them with switched $/\\epsilon /$ sounds.  Listen to the synthesized sounds and judge the success of the editing.\n",
    "\n",
    "**Hints**:\n",
    "- The numpy function *arange(a,b)* creates an array [a,a+1,a+2,...,b-1]\n",
    "- The numpy function *delete* removes indices from array\n",
    "- The numpy function *insert* can be used to insert values in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#\n",
    "# Solution:\n",
    "sound_s_new = np.insert(np.delete(sound_s,np.arange(start_s_e,end_s_e)),start_f_e,sound_f_e)\n",
    "sound_f_new = np.insert(np.delete(sound_f,np.arange(start_f_e,end_f_e)),start_f_e,sound_s_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio(sound_s_new, rate=SAMPLING_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Audio(sound_f_new, rate=SAMPLING_FREQUENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7 (optional)\n",
    "Challenge, can you synthesize the sound of the *word* sofa from the components of the single letter sounds? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#\n",
    "\n",
    "# Solution:\n",
    "sound_sofa = np.append(np.append(np.append(np.delete(sound_s,np.arange(0,end_s_e)),sound_o), \\\n",
    "                                 np.delete(sound_f,np.arange(0,end_f_e))), \\\n",
    "                               sound_a)\n",
    "                       \n",
    "Audio(sound_sofa, rate=SAMPLING_FREQUENCY)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
