{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 2, session 2\n",
    "# Decision theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume the following imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns;\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy import special\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from ipywidgets import interact,FloatSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Bayes' Theorem\n",
    "\n",
    "Recall, that for real variables $x$ and labels $C_k = 1,...c$, Bayes' theorem gives:\n",
    "\n",
    "$P(C_k|x) = \\dfrac{p(x|C_k)P(C_k)}{p(x)}  = \\dfrac{p(x|C_k)P(C_k)}{\\sum_j p(x|C_j)P(C_j)}$\n",
    "\n",
    "In this exercise session we will illustrate Bayes' theorem, by letting the class-conditional distributions be three normals with individual parameter sets. \n",
    "\n",
    "We define three univariate normal distributions $p_1, p_2, p_3$ to represent the three possible classes\n",
    " $C_1,C_2$ and $C_3$. The prior probabilities of the three classes are given by $P(C_1) = 0.4, P(C_2) = 0.3, P(C_3) = 0.3$. The distributions have different mean $\\mu_1 = -2, \\mu_2 = 0, \\mu_3 = 2$ while same variance $\\sigma^2_1 = \\sigma^2_2 = \\sigma^2_3 = 1$.\n",
    " \n",
    " ### Questions\n",
    " \n",
    " \n",
    "$\\star$ Plot the three class conditional density functions $P(x|C_1)$, $P(x|C_2)$ and $P(x|C_3)$ in the same figure, in the interval $-10 \\leq x \\leq 10$.\n",
    " \n",
    "$\\star$ Compute and plot the resulting density $p(x)$.\n",
    "\n",
    "$\\star$ Compute and plot the posterior probabilities $P(C_1|x), P(C_2|x), P(C_3|x)$ in the same figure.\n",
    "\n",
    "### Hints\n",
    "\n",
    "$\\bullet$ You can use the scipy.stats function norm.pdf to get the value of the normal probability density function in $x$ as:\n",
    "\n",
    "    px = norm.pdf(x,mean,sigma)\n",
    "    \n",
    "$\\bullet$ $p(x)$ can be computed using the law of total probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot densities P(x|c)\n",
    "\n",
    "#compute and plot the density p(x)\n",
    "\n",
    "#compute and plot posterior P(C|x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Simulate samples from the class conditional distribution\n",
    "\n",
    "In this exercise you will write a simulator that randomly draw samples from multiple normal distributions. We still consider the same three-class situation (with the same means, variances and priors as in the previous exercise).\n",
    "\n",
    "Sampling from the class conditional distribution $p(x)$ can be simulated by a generative process with two steps:\n",
    "\n",
    "- First: The class of the sample is determined randomly based on the prior probability for each class. \n",
    "- Second: The value of the sample is determined by a random draw from the normal distribution associated with the selected sample class.\n",
    "\n",
    "### Questions\n",
    "$\\star$ Define a function to draw samples from the class conditional distribution, simulating the generative process. The function must return two values, the sample-class and the sample-value. \n",
    "\n",
    "$\\star$ Draw 100,000 samples using the function and plot the histogram of the sample values in the same plot as the density $p(x)$.\n",
    "\n",
    "$\\star$ Comment on the resulting figure when you draw different number of samples.\n",
    "\n",
    "### Hints\n",
    "\n",
    "$\\bullet$ Consider using the numpy function np.random.choice for simulating the first step. For instance, the following line would simulate would simulate flipping a fair coin with sample space $S=\\{0,1 \\}$:\n",
    "   \n",
    "    flip = np.random.choice(2,1,p=[.5,.5])[0]\n",
    "    \n",
    "$\\bullet$ You can use the numpy function np.random.normal to simulate random draws from a normal distribution, as:\n",
    "\n",
    "    sampleValue = np.random.normal(loc = mean, scale = sigma)\n",
    "    \n",
    "$\\bullet$ If you draw the samples iteratively in a loop, you can store the individual sample results in predefined arrays, for instance initialized as:\n",
    "\n",
    "    N = 100000;\n",
    "    sampleValues = np.zeros(N);\n",
    "    sampleClasses = np.zeros(N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to simulate draws from the class conditional distribution\n",
    "\n",
    "#draw samples from the distribution\n",
    "\n",
    "#plot histogram of samples together with the density p(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Decision boundaries\n",
    "A decision rule, is a division of the space of $x$ so that each point is uniquely associated\n",
    "with a single class $C_k$.\n",
    "For instance, we can set up a 1D decision rule by dividing the space of $x$ into three intervals:\n",
    "$I_1 =\\; ] -\\infty, d_1]\\,, I_2 = \\; ] d_1, d_2]\\,, I_3 = \\;] d_2, \\infty [$.\n",
    "\n",
    "The errors of a decision rule can be summarized by an error confusion matrix $R$.\n",
    "In an experimental setting, the confusion matrix summarizes true class of the samples and what class they would have been assigned to according to the decision rule. In this way the confusion matrix can be used to evaluate the performance of the decision model, and is also known as an error matrix.\n",
    "\n",
    "We still consider the same three-class situation (with the same means, variances and priors as in the previous exercise). \n",
    "\n",
    "In this exercise we will display confusion matrices, such that each row represents the instances of a predicted class while each column represents the instances of an actual class.\n",
    "An element $R_{ij}$ in the matrix hence shows how many samples of class $i$ were assigned to class $j$ according to the decision rule. \n",
    "\n",
    "### Questions\n",
    "\n",
    "$\\star$ Plot the following two decision boundaries together with the posterior class probabilities.\n",
    "    \n",
    "$\\quad d_1 = -3.8\\;,\\quad d_2 = 1$\n",
    "\n",
    "$\\star$ Use your simulator to draw $N = 100,000$ samples. Record both the sample values and sample classes ('actual class').  For each sample also record the 'predicted class' (determined according to the decision rules). Store these records in two individual arrays of length $N$.\n",
    "\n",
    "$\\star$ Explain and comment on the two confusion matrices, computed by the code (using the sklearn.metrics function confusion_matrix):\n",
    "             \n",
    "         CM = confusion_matrix(predictedClasses,sampleClasses)\n",
    "         CM_normalized = CM / CM.sum()\n",
    "       \n",
    "### Hints\n",
    "$\\bullet$ The following code-snippets will plot the confusion matrices as heat-maps.\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define decision boundaries and plot together with the posterior class probabilities P(c|x)\n",
    "\n",
    "#sample from the distribution using the simulator function\n",
    "\n",
    "#compute and display confusion matrix\n",
    "CM = confusion_matrix(predictedClasses,sampleClasses)\n",
    "CM_normalized = CM / CM.sum()\n",
    "\n",
    "#plot CM as heatmap\n",
    "plt.title('confusion matrix (counts)')\n",
    "sns.heatmap(CM,xticklabels=['true1','true2','true3'],yticklabels=['pred1','pred2','pred3'],annot=True,cmap=\"Reds\", fmt = 'd')\n",
    "plt.show()\n",
    "#plot CM_normalized as heatmap\n",
    "plt.title('empirical confusion matrix')\n",
    "sns.heatmap(CM_normalized,xticklabels=['true1','true2','true3'],yticklabels=['pred1','pred2','pred3'],annot=True,cmap=\"Reds\",fmt=\".5f\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Decision process\n",
    "Each element in the confusion matrix $R$ can be analytically computed as: $R_{jk} = \\int_{I_j} p(x|C_k) dx$. \n",
    "\n",
    "We still consider the same three-class situation (with the same means, variances and priors as in the previous exercise). \n",
    "\n",
    "Execute the following codeblock. It will show an interactive widget. It allows you to change the decision boundries and priors for the three distributions, and will plot corresponding figures including the resulting analytically computed confusion matrix.\n",
    "\n",
    "### Questions\n",
    "\n",
    "$\\star$ Comment on how the posterior class probability $P(c|x)$ and density $p(x)$ plot changes for different prior configurations.\n",
    "\n",
    "$\\star$ Can you create a situation in which a class has no points $x$ where it is most probable? How would such a situation influence a decision process?\n",
    "\n",
    "$\\star$ How does the colored area in the first subfigure relate to the classification error?\n",
    "\n",
    "$\\star$ Comment on how you can find an 'optimal' decision rule (by changing $d_1$ and $d_2$ and monitoring the confusion matrix).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define means, variances and sigmas for the three normals \n",
    "means = [-2, 0, 2];\n",
    "variances = [1, 1, 1];\n",
    "sigmas = np.sqrt(variances);\n",
    "\n",
    "#function to compute confusion matrix\n",
    "def getConfusionMatrix(d1, d2, prior1,prior2,prior3):\n",
    "    \n",
    "    #compute the confusion matrix for intervals: ]-inf, d1] , ]d1, d2] , ]d2,inf[\n",
    "    R = np.array([np.zeros(3),np.zeros(3),np.zeros(3)])    \n",
    "    R[0,0] = norm(means[0],sigmas[0]).cdf(d1)    \n",
    "    R[0,1] = norm(means[1],sigmas[1]).cdf(d1)\n",
    "    R[0,2] = norm(means[2],sigmas[2]).cdf(d1)    \n",
    "    R[1,0] = norm(means[0],sigmas[0]).cdf(d2) - norm(means[0],sigmas[0]).cdf(d1)\n",
    "    R[1,1] = norm(means[1],sigmas[1]).cdf(d2) - norm(means[1],sigmas[1]).cdf(d1)\n",
    "    R[1,2] = norm(means[2],sigmas[2]).cdf(d2) - norm(means[2],sigmas[2]).cdf(d1)\n",
    "    R[2,0] = 1-norm(means[0],sigmas[0]).cdf(d2)    \n",
    "    R[2,1] = 1-norm(means[1],sigmas[1]).cdf(d2)\n",
    "    R[2,2] = 1-norm(means[2],sigmas[2]).cdf(d2)        \n",
    "\n",
    "    return R * np.array([prior1,prior2,prior3]);\n",
    "\n",
    "#function to draw posterior class probabilites, decision boundaries and confusion matrix in same plot\n",
    "def drawDecisionProcess(d1,d2,prior1,prior2,prior3):    \n",
    "    \n",
    "    #normalize the priors (to sum to 1)\n",
    "    prior_sum = prior1 + prior2 + prior3    \n",
    "    prior1 = prior1/prior_sum;\n",
    "    prior2 = prior2/prior_sum;\n",
    "    prior3 = prior3/prior_sum;    \n",
    "    print('P(C1) = ',prior1)\n",
    "    print('P(C2) = ',prior2)\n",
    "    print('P(C3) = ',prior3)\n",
    "    \n",
    "    x = np.linspace(-10,10, 100)\n",
    "    px_c1 = norm.pdf(x,means[0],sigmas[0]);\n",
    "    px_c2 = norm.pdf(x,means[1],sigmas[1]);\n",
    "    px_c3 = norm.pdf(x,means[2],sigmas[2]);\n",
    "\n",
    "    fig = plt.figure(num=None, figsize=(26, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "    \n",
    "    ax1 = fig.add_subplot(221) \n",
    "    ax1.set_title('joint probability distributions p(x|c)*P(c)')\n",
    "    ax1.plot(x, prior1 * px_c1)\n",
    "    ax1.plot(x, prior2 * px_c2)\n",
    "    ax1.plot(x, prior3 * px_c3)\n",
    "    ax1.plot([d1,d1],[0,0.16],'k')\n",
    "    ax1.plot([d2,d2],[0,0.16],'k')\n",
    "    \n",
    "    \n",
    "    \n",
    "    fill_style = {'color': 'r', 'alpha': 0.1}\n",
    "    xmistake = np.linspace(d1,10,50)\n",
    "    ax1.fill_between(xmistake,np.zeros_like(xmistake),prior1 * norm.pdf(xmistake,means[0],sigmas[0]),**fill_style)\n",
    "    \n",
    "    xmistake = np.linspace(-10,d1,50)\n",
    "    ax1.fill_between(xmistake,np.zeros_like(xmistake),prior2 * norm.pdf(xmistake,means[1],sigmas[1]),**fill_style)\n",
    "    xmistake = np.linspace(d2,10,50)\n",
    "    ax1.fill_between(xmistake,np.zeros_like(xmistake),prior2 * norm.pdf(xmistake,means[1],sigmas[1]),**fill_style)\n",
    "    \n",
    "    xmistake = np.linspace(-10,d2,50)\n",
    "    ax1.fill_between(xmistake,np.zeros_like(xmistake),prior3 * norm.pdf(xmistake,means[2],sigmas[2]),**fill_style)\n",
    "    \n",
    "    #plot P(c|x)\n",
    "    px = px_c1 * prior1 + px_c2 * prior2 + px_c3 * prior3;\n",
    "    P1x = px_c1*prior1/px;\n",
    "    P2x = px_c2*prior2/px;\n",
    "    P3x = px_c3*prior3/px;\n",
    "    \n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax2.set_title('posterior class probabilities P(c|x)')\n",
    "    ax2.plot(x,P1x)\n",
    "    ax2.plot(x,P2x)\n",
    "    ax2.plot(x,P3x)\n",
    "    ax2.plot([d1,d1],[0,1],'k')\n",
    "    ax2.plot([d2,d2],[0,1],'k')\n",
    "    \n",
    "    #plot p(x)\n",
    "    ax3 = fig.add_subplot(223) \n",
    "    ax3.set_title('density p(x)')\n",
    "    ax3.plot(x, px)\n",
    "     \n",
    "    #plot confusion matrix\n",
    "    R = getConfusionMatrix(d1,d2,prior1,prior2,prior3);\n",
    "    \n",
    "    ax4 = fig.add_subplot(224) \n",
    "    \n",
    "    sns.heatmap(R,xticklabels=['true1','true2','true3'],yticklabels=['pred1','pred2','pred3'],annot=True,fmt=\".5f\",cmap=\"Reds\")\n",
    "    ax4.set_title('confusion matrix')\n",
    "    \n",
    "interact(drawDecisionProcess, \n",
    "\td1=FloatSlider(min = -10.0, max = 0.0, value = -3.8, continuous_update=False),\n",
    "\td2 = FloatSlider(min = 0.0, max = 10, value = 1.0, continuous_update = False),\n",
    "\tprior1 = FloatSlider(min = 0.0, max = 1, step = .01, value = 0.4, continuous_update = False),\n",
    "\tprior2 = FloatSlider(min = 0.0, max = 1, step = .01, value = 0.3, continuous_update = False),\n",
    "\tprior3 = FloatSlider(min = 0.0, max = 1, step = .01, value = 0.3, continuous_update = False),\n",
    ");    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
