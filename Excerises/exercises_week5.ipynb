{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum likelihood estimation\n",
    "\n",
    "Let observed data points $\\mathbf{x} = (x_1,\\ldots,x_N)$ have the joint density function $p(x_1,\\ldots,x_N ; \\theta)$. The likelihood function is defined as a function of the model parameters, $\\theta$, equivalent to the density of the observed data:\n",
    "\n",
    "$$\n",
    " L(\\theta; x_1,\\ldots,x_N) = p(x_1,\\ldots,x_N | \\theta) \n",
    "$$\n",
    "\n",
    "The likelihood describes the support by the data for each possible configuration of the parameters. \n",
    "If we compare the likelihood function for two different sets of parameters, $\\theta_1$ and $\\theta_2$, and find that $L(\\theta_1;\\mathbf{x}) > L(\\theta_2;\\mathbf{x})$, we can conclude that the observed data is more likely according to the model with parameters $\\theta_1$ than the model with parameters $\\theta_2$. \n",
    "\n",
    "'Maximum likelihood estimation' utilizes this principle as a method for estimating appropriate model parameters based on the observations alone. In a generative setting, we are concerned with estimating the set of parameters $\\hat{\\theta}$ that describes the model which can be assumed to have generated the observations $\\mathbf{x}$. These parameter estimates are obtained by maximizing the likelihood function with regard to $\\theta$: \n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\arg\\max_\\theta p(\\mathbf{x}|\\theta)\n",
    "$$\n",
    "\n",
    "Often it is both mathematically and computationally convenient to maximize the natural logarithm of the likelihood function (called the log-likelihood) instead. Solving this maximization problem is similar to the original one because the logarithm is a monotonic (strictly increasing) function. Many machine learning and optimization algorithms prefer to minimize functions (such as working with cost-functions) in which case the problem can be solved by minimize the negative log-likelihood instead.\n",
    "\n",
    "\n",
    "### Questions\n",
    "In this exercise we illustrate the method of maximum likelihod as an estimator for the parameters (mean and variance) of the normal distribution. \n",
    "\n",
    "Recall that the normal probability density function with mean $\\mu$ and variance $\\sigma^2$ of a given sample $x_i$ is:\n",
    "\n",
    "$$\n",
    "p(x_i;\\mu,\\sigma^2) = (2\\pi\\sigma^2)^{-1/2} \\exp\\left( - \\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "$\\star$ Assuming that all observations $\\mathbf{x} = (x_1,\\ldots x_N)$ are independent and identically distributed, derive the likelihood function,\n",
    "\n",
    "$$\n",
    "L(\\mu,\\sigma^2;x_1,\\ldots x_N) = \\left(2\\pi\\sigma^2\\right)^{-N/2} \\exp\\left( - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (x_i - \\mu)^2 \\right)\n",
    "$$\n",
    "\n",
    "$\\star$ Generate a data set of $N=5$ random points for the normal distribution with true parameters $\\mu_0= 1$ and $\\sigma_{0}^2 = 0.5$. Assume that you know that the variance is $\\sigma^2 = \\sigma_{0}^2 = 0.5$. \n",
    "- Plot the likelihood for different values of $\\mu$ in the range $\\mu = [-1, 10]$. Comment on the figure. Repeat the experiment for $N = 30$.\n",
    "- For $N$ in the range $N = [2,30]$, compute the $\\textit{estimates}$ for mean and variance (sample mean and sample variance). Plot these as function of $N$ and compare with the true parameter values.\n",
    "\n",
    "$\\star$ By taking the natural logarithm of the likelihood function, convince yourself that the log-likelihood is:\n",
    "\n",
    "$$\n",
    "\\ln L(\\mu,\\sigma^2;x_1,\\ldots x_N) = - \\frac{N}{2}\\ln\\left(2\\pi\\right) - \\frac{N}{2}\\ln\\left(\\sigma^2\\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}\\left(x_j - \\mu\\right)^2\n",
    "$$\n",
    "\n",
    "$\\star$ Prove that the maximum likelihood estimators of the mean and variance equals the $\\textit{sample mean}$ and $\\textit{sample variance}$;\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{N}\\sum_{i = 1}^N\\left(x_i \\right) \\quad,\\quad\n",
    "\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{i = 1}^N(x_i - \\hat{\\mu})^2.\n",
    "$$\n",
    "\n",
    "This can be done by solving the maximization problem\n",
    "\n",
    "$$\n",
    "\\max_{\\mu,\\sigma^2} \\ln L(\\mu,\\sigma^2;\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "The proof can be conducted by taking the partial derivatives of the log-likelihood with respect to the mean and variance, equate to zero and solve for $\\mu$ and $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\mu}\\ln L(\\mu,\\sigma^2;\\mathbf{x}) = 0  \\qquad,\\qquad\n",
    "\\frac{\\partial}{\\partial\\sigma^2}\\ln L(\\mu,\\sigma^2;\\mathbf{x}) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "### Hints\n",
    "$\\bullet$ Given the assumption that observations are independent and identically distributed, the likelihood function can be written as the product of the PDF evaluated for the individual samples:\n",
    "\n",
    "$$\n",
    "L(\\theta;x_1,\\ldots,x_N) = \\prod_{i=1}^N p(x_i;\\theta)\n",
    "$$\n",
    "\n",
    "$\\bullet$ Recall the rules and properties for logarithms:\n",
    "\n",
    "$$\n",
    "\\ln(a^b) = b\\ln(a)\\quad,\\quad \\ln(\\exp(a)) = a \\quad,\\quad \\ln(a\\cdot b) = \\ln(a)+\\ln(b)\n",
    "$$\n",
    "\n",
    "$\\bullet$ The solutions to the partial derivatives of the log-likelihood are:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\mu}\\ln L(\\mu,\\sigma^2;\\mathbf{x}) = \\frac{1}{\\sigma^2}\\sum_{i=1}^N\\left(x_i-\\mu \\right)  \n",
    "\\quad,\\quad\n",
    "\\frac{\\partial}{\\partial\\sigma^2}\\ln L(\\mu,\\sigma^2;\\mathbf{x}) = \\frac{1}{2\\sigma^2}\\left( \\frac{1}{\\sigma^2}\\sum_{i=1}^N\\left(x_i-\\mu\\right)^2-N\\right)\n",
    "$$\n",
    "To solve the maximization problem, consider when these are equal to zero. You can ignore $\\sigma^2 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordinate transformation\n",
    "\n",
    "In practical machine-learning it is often desired that the input data has zero mean, unit variance and covariance. When normalizing data to uphold this, it is often possible to use the same algorithms (without changing the control parameters) on data originating from different signal-sources and with different covariation.\n",
    "\n",
    "Geometrically, such a normalization corresponds to a coordinate transformation to a system that is defined by the eigenvectors of the covariance matrix. Typically, the mean and covariance matrix are not known, and must therefore be estimated from the data-set.\n",
    "\n",
    "Given the dataset $D = \\{\\mathbf{x}_1,\\mathbf{x}_2,...\\mathbf{x}_N\\}$, the estimated mean and covariance matrix are given by:\n",
    "$$ \\hat{\\mathbf{x}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{x}_i$$\n",
    "$$ \\hat{\\mathbf{\\Sigma}} = \\frac{1}{N}\\sum_{i=1}^{N}(\\mathbf{x}_i-\\hat{\\mathbf{x}})(\\mathbf{x}_i-\\hat{\\mathbf{x}})^T$$\n",
    "\n",
    "The eigenvalue equation for the covariance matrix is\n",
    "\n",
    "$$ \\hat{\\mathbf{\\Sigma}} \\mathbf{u}_j = \\lambda_j\\mathbf{u}_j ,\\quad j = 1,...,d.$$\n",
    "\n",
    "Where $\\lambda_j$ is the $j$'th eigenvalue and $\\mathbf{u}_j$ is the corresponding eigenvector of $\\hat{\\mathbf{\\Sigma}}$, such that the transformed input variables are given by\n",
    "\n",
    "$$ \\tilde{\\mathbf{x}} = \\Lambda^{-1/2}\\mathbf{U}^T (\\mathbf{x_i}-\\hat{\\mathbf{x}}), $$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\mathbf{U} = (\\mathbf{u}_1,...,\\mathbf{U}_d)$$\n",
    "\n",
    "$$\\Lambda = \\text{diag}(\\lambda_1,...,\\lambda_d).$$\n",
    "\n",
    "It can be shown that the transformed dataset, $\\tilde{D} = (\\tilde{\\mathbf{x}}_1,\\tilde{\\mathbf{x}}_2,...,\\tilde{\\mathbf{x}}_N,) $ has zero mean and a covariance matrix given by the unit matrix.\n",
    "\n",
    "### Questions\n",
    "\n",
    "$\\star$ Generate a dataset $D$ by drawing $N = 1000$ random samples from the 2-dimensional multivariate normal PDF\n",
    "\n",
    "$$ \\mathcal{N}(\\mathbf{x}; \\mathbf{\\mu},\\mathbf{\\Sigma}),\\quad \\text{where }\\mathbf{\\mu} = (2,5)\\;\\text{and}\\;\\mathbf{\\Sigma} \n",
    "= \\begin{bmatrix}2 & -1.2\\\\ -1.2 &3\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\star$ Show the empiric data, $D$, in a scatter plot and compute the estimated mean, $\\hat{\\mathbf{\\mu}}$, and covariance matrix, $\\hat{\\mathbf{\\Sigma}}$. You can use the np.mean and np.cov functions (np.cov has an optional  $\\textit{bias}$ option, what should this be?). \n",
    "\n",
    "$\\star$ Compute and show the eigenvectors, $\\mathbf{U}$, as arrows on top of the scatter plot. Eigenvalues and eigenvectors can be computed using the np.linalg.eig function. \n",
    "- Comment on the geometrical significance of the eigenvalues and eigenvectors.\n",
    "\n",
    "$\\star$ Compute the transformed dataset, $\\tilde{D}$. Illustrate and comment on the transformation.\n",
    "\n",
    "$\\star$ What happens if the term $\\Lambda^{-1/2}$ is removed from the equation for obtaining the transformed input variables $\\tilde{\\mathbf{x}}_i$? \n",
    "- Illustrate this by making a scatter plot of the transformed data and show the resulting eigenvectors when $\\Lambda^{-1/2}$ is omitted.\n",
    "\n",
    "\n",
    "$\\star$ Compare the transformed datasets from different distributions, by changing $\\mathbf{\\mu}$ and $\\mathbf{\\Sigma}$.\n",
    "\n",
    "### Hints\n",
    "\n",
    "$\\bullet$ Use the plt.scatter function to draw a 2-dimensional scatter plot.\n",
    "\n",
    "$\\bullet$ Use plt.axis('equal') to get equal axis ratios, which can make your plots easier to interpret.\n",
    "\n",
    "$\\bullet$ You can plot an arrow between two points (x1,y1) and (x2,y2) using the plt.arrow function\n",
    "    \n",
    "    plt.arrow(x1,y1,x2-x1,y2-y1, head_width = .4, length_includes_head = True)\n",
    "\n",
    "\n",
    "$\\bullet$ The diagonal matrix $\\Lambda^{-1/2}$ can be created by \n",
    "\n",
    "    L = np.sqrt(np.linalg.inv(np.diag([lambda1,lambda2])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection on Eigenvectors\n",
    "\n",
    "In some cases, the measured data is of a lower effective dimension than the apparent dimension of the data vector. For example, imagine a data-set of a 3-dimensional variable. If all the data are on a straight line, and data can be described in only 1D. If the data-set is transformed to a coordinate system, where the variation of the data is along one of the axes, the two other components can be ignored, as the variation of the original signal can be described by the single component.\n",
    "\n",
    "Let $\\lambda_1,\\ldots,\\lambda_d$ be the ordered set of eigenvalues of the covariance matrix, such that $\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_d $. If there exists a number $m$, such that $\\lambda_i \\gg \\lambda_j, i = 1,\\ldots m$, and $j=m+1,\\ldots,d$, then the data-set can be transformed to a coordinate system, where most of the signal variance is in an $m$-dimensional linear subspace spanned by the $m$'th first eigenvectors in the ordered list. This transformation is again given by the eigenvectors of the covariance matrix ($\\bf U$):\n",
    "\n",
    "$$  \\tilde{\\mathbf{x}}_i = \\mathbf{U}^{\\top}(\\mathbf{x}_i - \\widehat{\\mathbf{x}}). $$\n",
    "\n",
    "If we extract only the first $m$ components of the transformed datavector $\\tilde{\\bf x}$ we obtain a signal that carries most of the variation of the original signal. Such reduction of the effective dimensionlity of the problem is also known as extraction of features.\n",
    "\n",
    "\n",
    "\n",
    "### Questions \n",
    "\n",
    "In this exercise you will visualize the dimensionality reduction for a 2D classification problem.\n",
    "\n",
    "Consider all data points as pairs $(x_1,x_2)$ in the the datavector $\\mathbf{x}$ generated randomly from the two classes $C_1$ and $C_2$, with prior probabilities $P(C_1) = 0.3, P(C_2) = 0.7$.\n",
    "\n",
    "Let samples from $C_1$ be based on a 2D normal distribution with parameters \n",
    "\n",
    "$$ \\mathbf{\\mu}_1 = (1.6,2.4)\\qquad\\text{and}\\qquad\\mathbf{\\Sigma}_1 \n",
    "= \\begin{bmatrix} 1.2 & 1\\\\ 1 &4\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and let samples from $C_2$ be based on a 2D normal distribution with parameters\n",
    "\n",
    "$$ \\mathbf{\\mu}_2 = (-2.2,-3.3),\\quad\\text{and},\\quad\\mathbf{\\Sigma}_2\n",
    "= \\begin{bmatrix} 2 & -1.4\\\\ -1.4 &2\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\star$ Write a sampler that simulates draws from the class conditional distribution. This can be done as a two-step generative process, where you first draw the class (according to the prior probabilities for each class) and then draw the sample value based on the class.\n",
    "\n",
    "- Generate a dataset $D = (\\mathbf{x}_1,\\mathbf{x}_2)$ by drawing $N=10000$ samples. \n",
    "- Illustrate $D$ in a scatter plot. Compute and show the eigenvectors in the plot.\n",
    "- Show the histograms for the two marginal distributions ( $p(\\mathbf{x}_1)$ and $p(\\mathbf{x}_2)$) based on the samples.\n",
    "\n",
    "\n",
    "$\\star$ Perform the coordinate transformation into the eigenvector space.\n",
    "- Ilustrate the transformed data-vector $\\widehat{\\bf x}$ in a 2D scatter plot. Compute and show the eigenvectors of the transformed data-set in the plot.\n",
    "- Compute the mean and covariance after the transformation and confirm that they are as expected.\n",
    "- Show the histograms for the marginals after the transformation ($p(\\tilde{\\mathbf{x}}_1)$ and $p(\\tilde{\\mathbf{x}}_2)$).\n",
    "- comment on the true dimensionality of the data\n",
    "\n",
    "$\\star$ Repeat the experiment when the two normals are defined by the following parameters:\n",
    "\n",
    "$$ \\mathbf{\\mu}_1 = (1,6)\\qquad \\mathbf{\\mu}_2 = (-6,-1) \\qquad\\text{and}\\qquad\\mathbf{\\Sigma}_1\n",
    "= \\mathbf{\\Sigma}_2 = \\begin{bmatrix} 1 & 0\\\\ 0 &1\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "### Hints\n",
    "\n",
    "$\\bullet$ If you store the data points in two arrays, $\\textsf{datax}$ and $\\textsf{datay}$, a two dimensional data matrix can be created as\n",
    "\n",
    "    data = np.array((datax,datay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
